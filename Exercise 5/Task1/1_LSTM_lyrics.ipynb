{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6cbd630f",
      "metadata": {
        "id": "6cbd630f"
      },
      "source": [
        "# Exercise 1: LSTM for Generating Text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3917ee78",
      "metadata": {
        "id": "3917ee78"
      },
      "source": [
        "\n",
        "This exercise demonstrates the use of an LSTM (Long Short-Term Memory) neural network to generate text based on song lyrics. The process includes loading song lyrics, tokenizing the text, creating sequences, training the LSTM model, and generating new lyrics based on a given seed sentence.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0616baaf",
      "metadata": {
        "id": "0616baaf"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Load the lyrics\n",
        "def load_lyrics(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        lyrics = f.read()\n",
        "    return lyrics.lower()\n",
        "\n",
        "lyrics = load_lyrics('./bieber.txt')  # Provide path to your lyrics file\n",
        "\n",
        "# Tokenize and Vectorize the Text\n",
        "from collections import Counter\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)  # Remove punctuation\n",
        "    words = text.split()\n",
        "    return words\n",
        "\n",
        "words = preprocess_text(lyrics)\n",
        "word_count = Counter(words)\n",
        "vocab = sorted(word_count, key=word_count.get, reverse=True)\n",
        "vocab_to_int = {word: i for i, word in enumerate(vocab, 1)}\n",
        "\n",
        "# Convert the lyrics to a sequence of integers\n",
        "lyrics_int = [vocab_to_int[word] for word in words]\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "75e10a69",
      "metadata": {
        "id": "75e10a69"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create n-grams and pad sequences\n",
        "def create_sequences(lyrics_int, seq_length):\n",
        "    sequences = []\n",
        "    for i in range(seq_length, len(lyrics_int)):\n",
        "        seq = lyrics_int[i-seq_length:i+1]\n",
        "        sequences.append(seq)\n",
        "    return sequences\n",
        "\n",
        "seq_length = 10\n",
        "sequences = create_sequences(lyrics_int, seq_length)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "03aaf606",
      "metadata": {
        "id": "03aaf606"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create Predictors and Labels\n",
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:, :-1], sequences[:, -1]\n",
        "X = torch.tensor(X)\n",
        "y = torch.tensor(y)\n",
        "\n",
        "# One-hot encode labels\n",
        "y = torch.nn.functional.one_hot(y, num_classes=len(vocab_to_int) + 1).float()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7d1b2cab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d1b2cab",
        "outputId": "a33d9697-be25-47bc-b5f1-6463d9d0c1e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|██████████| 459/459 [00:32<00:00, 14.19it/s, Loss=3.18, Accuracy=0.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 Summary: \n",
            "Average Loss: 5.2889, Accuracy: 0.1200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 459/459 [00:33<00:00, 13.61it/s, Loss=3.6, Accuracy=0.287]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 Summary: \n",
            "Average Loss: 3.8596, Accuracy: 0.2873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 459/459 [00:33<00:00, 13.78it/s, Loss=2.86, Accuracy=0.43]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 Summary: \n",
            "Average Loss: 2.9057, Accuracy: 0.4296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 459/459 [00:32<00:00, 13.99it/s, Loss=2.48, Accuracy=0.558]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 Summary: \n",
            "Average Loss: 2.1742, Accuracy: 0.5578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 459/459 [00:31<00:00, 14.38it/s, Loss=1.7, Accuracy=0.668]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20 Summary: \n",
            "Average Loss: 1.6092, Accuracy: 0.6680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 459/459 [00:32<00:00, 14.07it/s, Loss=0.521, Accuracy=0.765]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20 Summary: \n",
            "Average Loss: 1.1698, Accuracy: 0.7648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 459/459 [00:32<00:00, 14.22it/s, Loss=1.28, Accuracy=0.835]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20 Summary: \n",
            "Average Loss: 0.8433, Accuracy: 0.8345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 459/459 [00:31<00:00, 14.45it/s, Loss=0.469, Accuracy=0.886]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20 Summary: \n",
            "Average Loss: 0.6012, Accuracy: 0.8863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 459/459 [00:32<00:00, 13.98it/s, Loss=0.546, Accuracy=0.927]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20 Summary: \n",
            "Average Loss: 0.4233, Accuracy: 0.9266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 459/459 [00:32<00:00, 13.92it/s, Loss=0.396, Accuracy=0.952]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20 Summary: \n",
            "Average Loss: 0.3022, Accuracy: 0.9516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 459/459 [00:32<00:00, 13.95it/s, Loss=0.457, Accuracy=0.965]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20 Summary: \n",
            "Average Loss: 0.2236, Accuracy: 0.9645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 459/459 [00:31<00:00, 14.39it/s, Loss=0.149, Accuracy=0.971]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20 Summary: \n",
            "Average Loss: 0.1745, Accuracy: 0.9708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 459/459 [00:32<00:00, 13.94it/s, Loss=0.0803, Accuracy=0.973]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20 Summary: \n",
            "Average Loss: 0.1456, Accuracy: 0.9730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 459/459 [00:31<00:00, 14.37it/s, Loss=0.168, Accuracy=0.975]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20 Summary: \n",
            "Average Loss: 0.1269, Accuracy: 0.9750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 459/459 [00:32<00:00, 13.97it/s, Loss=0.248, Accuracy=0.975]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20 Summary: \n",
            "Average Loss: 0.1192, Accuracy: 0.9746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 459/459 [00:32<00:00, 14.25it/s, Loss=0.0514, Accuracy=0.975]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20 Summary: \n",
            "Average Loss: 0.1126, Accuracy: 0.9751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 459/459 [00:33<00:00, 13.58it/s, Loss=0.0438, Accuracy=0.975]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20 Summary: \n",
            "Average Loss: 0.1124, Accuracy: 0.9746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 459/459 [00:32<00:00, 14.18it/s, Loss=0.714, Accuracy=0.974]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20 Summary: \n",
            "Average Loss: 0.1124, Accuracy: 0.9740\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 459/459 [00:32<00:00, 14.19it/s, Loss=0.0396, Accuracy=0.975]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20 Summary: \n",
            "Average Loss: 0.1090, Accuracy: 0.9745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 459/459 [00:32<00:00, 13.91it/s, Loss=0.0202, Accuracy=0.975]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20 Summary: \n",
            "Average Loss: 0.1014, Accuracy: 0.9755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from termcolor import colored\n",
        "import numpy as np\n",
        "\n",
        "# Build and Train the LSTM Model\n",
        "class LSTMLyricsModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(LSTMLyricsModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        out = self.fc(lstm_out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = len(vocab_to_int) + 1\n",
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "output_dim = vocab_size\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = LSTMLyricsModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def accuracy(preds, labels):\n",
        "    _, pred_labels = torch.max(preds, 1)  # Get the index of the max log-probability\n",
        "    correct = (pred_labels == labels).sum().item()  # Compare predictions and true labels\n",
        "    return correct / labels.size(0)\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, X, y, epochs=20, batch_size=64):\n",
        "    dataset = TensorDataset(X, y)  # Ensure y are integer labels, not one-hot\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        epoch_acc = 0.0\n",
        "\n",
        "        progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for batch_X, batch_y in progress_bar:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch_X)  # Forward pass\n",
        "            loss = loss_fn(output, batch_y)  # Compute loss\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            # Accumulate loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += accuracy(output, batch_y)\n",
        "\n",
        "            # Update progress bar with loss and accuracy\n",
        "            progress_bar.set_postfix({\n",
        "                'Loss': loss.item(),\n",
        "                'Accuracy': epoch_acc / len(loader)\n",
        "            })\n",
        "\n",
        "        # Print colored summary at the end of each epoch\n",
        "        print(colored(f\"Epoch {epoch+1}/{epochs} Summary: \", 'green'))\n",
        "        print(colored(f\"Average Loss: {epoch_loss / len(loader):.4f}, Accuracy: {epoch_acc / len(loader):.4f}\", 'yellow'))\n",
        "\n",
        "# Remove one-hot encoding from labels\n",
        "# Make sure the labels `y` are integers, not one-hot encoded vectors\n",
        "y = torch.tensor(sequences[:, -1], dtype=torch.long)  # The last word as label\n",
        "\n",
        "# Train the model\n",
        "train_model(model, X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5de546cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5de546cb",
        "outputId": "3e010a23-2f3a-4a7e-c3f6-5eacafff384c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in the vocabulary: ['you', 'i', 'the', 'me', 'to', 'and', 'oh', 'my', 'im', 'it', 'that', 'be', 'love', 'dont', 'in', 'baby', 'your', 'yeah', 'a', 'no', 'all', 'one', 'we', 'girl', 'like', 'do', 'know', 'is', 'with', 'what', 'for', 'but', 'youre', 'so', 'on', 'cause', 'up', 'make', 'need', 'if', 'right', 'now', 'got', 'just', 'when', 'never', 'its', 'can', 'go', 'let', 'of', 'time', 'want', 'wanna', 'say', 'ill', 'down', 'this', 'us', 'only', 'she', 'smile', 'cant', 'heart', 'see', 'get', 'aint', 'tell', 'back', 'out', 'are', 'was', 'whoa', 'could', 'gonna', 'at', 'life', 'have', 'ooh', 'theres', 'nothing', 'should', 'hey', 'how', 'world', 'not', 'gotta', 'give', 'as', 'mean', 'here', 'take', 'would', 'mind', 'where', 'way', 'believe', 'thats', 'from', 'had', 'away', 'without', 'around', 'were', 'will', 'less', 'better', 'show', 'they', 'somebody', 'ever', 'madly', 'crazy', 'been', 'alright', 'live', 'more', 'think', 'day', 'about', 'mine', 'her', 'am', 'lonely', 'eyes', 'there', 'doing', 'thought', 'always', 'through', 'keep', 'still', 'every', 'than', 'everything', 'na', 'earth', 'first', 'long', 'coming', 'said', 'come', 'ya', 'shes', 'or', 'babe', 'over', 'nobody', 'gone', 'id', 'wont', 'too', 'an', 'favorite', 'lose', 'much', 'by', 'feel', 'body', 'shawty', 'next', 'sometimes', 'our', 'leave', 'another', 'didnt', 'unless', 'then', 'put', 'tonight', 'fight', 'look', 'truth', 'youd', 'yourself', 'hard', 'really', 'hand', 'well', 'together', 'best', 'ima', 'fall', 'between', 'hold', 'ive', 'trust', 'hearts', 'left', 'alone', 'lets', 'dream', 'fly', 'pray', 'thing', 'home', 'did', 'good', 'stress', 'call', 'maybe', 'sorry', 'sky', 'standing', 'known', 'sleep', 'going', 'under', 'people', 'sense', 'stay', 'pick', 'worth', 'close', 'nowhere', 'night', 'forever', 'who', 'chance', 'eenie', 'meenie', 'miney', 'enough', 'kiss', 'why', 'find', 'ah', 'lifesaver', 'uhuh', 'trying', 'many', 'someone', 'made', 'wrong', 'them', 'side', 'lover', 'again', 'matter', 'tryna', 'try', 'whats', 'other', 'face', 'mo', 'wish', 'living', 'feels', 'each', 'change', 'verse', 'any', 'chorus', 'times', 'que', 'nothin', 'ohehohohoh', 'ohehoh', 'same', 'work', 'wind', 'little', 'company', 'gon', 'mistletoe', 'feeling', 'something', 'rush', 'move', 'wanted', 'breath', 'till', 'forget', 'fool', 'rock', 'beat', 'off', 'moment', 'tu', 'money', 'bring', 'em', 'promise', 'fast', 'pain', 'touch', 'these', 'beautiful', 'learn', 'everythings', 'whos', 'latin', 'catch', 'yours', 'open', 'woah', 'heaven', 'god', 'arms', 'has', 'making', 'ii', 'start', 'guy', 'set', 'words', 'hear', 'quiero', 'late', 'both', 'care', 'holdin', 'felt', 'real', 'pull', 'worry', 'uh', 'pressure', 'things', 'before', 'he', 'miss', 'head', 'place', 'perfect', 'lady', 'else', 'easy', 'car', 'die', 'turn', 'broken', 'deep', 'run', 'whenever', 'friends', 'thinking', 'lie', 'youll', 'looking', 'cry', 'makes', 'stand', 'walk', 'sure', 'fear', 'floor', 'tight', 'far', 'waiting', 'feelings', 'yes', 'running', 'until', 'tripping', 'text', 'girls', 'feelin', 'despacito', 'sube', 'myself', 'missing', 'anything', 'phone', 'new', 'knew', 'while', 'please', 'free', 'strong', 'cold', 'falling', 'help', 'hurt', 'half', 'others', 'young', 'driving', 'wrapped', 'justin', 'party', 'beauty', 'matters', 'end', 'took', 'two', 'wake', 'knock', 'some', 'last', 'y', 'de', 'bad', 'even', 'man', 'boyfriend', 'talk', 'already', 'nenever', 'stop', 'honestly', 'complete', 'understand', 'scared', 'stuck', 'lights', 'different', 'confident', 'might', 'bieber', 'wouldve', 'warm', 'bass', 'saying', 'magic', 'madness', 'lost', 'trippin', 'te', 'tus', 'pasito', 'poquito', 'hope', 'broke', 'buy', 'came', 'above', 'name', 'hit', 'mama', 'break', 'lookin', 'inside', 'across', 'snow', 'because', 'youve', 'sun', 'sayin', 'teach', 'pretty', 'til', 'answer', 'meet', 'loved', 'mark', 'theyre', 'wantwant', 'gotgot', 'catching', 'dirty', 'reason', 'into', 'guess', 'waste', 'slow', 'yo', 'mistakes', 'second', 'cool', 'pieces', 'rain', 'song', 'somethin', 'told', 'music', 'whole', 'issues', 'dance', 'him', 'his', 'moon', 'makin', 'angel', 'listen', 'thousand', 'everyday', 'working', 'being', 'seen', 'light', 'common', 'denominator', 'hate', 'swear', 'playin', 'winter', 'past', 'storm', 'drive', 'sorrow', 'fights', 'question', 'happy', 'point', 'air', 'remember', 'bout', '1', 'prechorus', 'fine', '2', 'finally', 'sent', 'write', 'found', 'viable', 'term', 'mi', 'favorito', 'those', 'single', 'apart', 'done', 'whatever', 'play', 'fix', 'breaking', 'own', 'true', 'okay', 'big', 'used', 'hands', 'hook', 'arm', 'imma', 'shine', 'goin', 'ey', 'ground', 'plus', 'nah', 'wouldnt', 'giving', 'lifes', 'lips', 'christmas', 'special', 'room', 'damn', 'ass', 'deserve', 'difference', 'weve', 'supposed', 'biggest', 'insane', 'relevant', 'aw', 'women', 'helps', 'lililive', 'hell', 'argue', 'throw', 'since', 'morning', 'closer', 'top', 'follow', 'el', 'la', 'couple', 'diamond', 'likes', 'ride', 'outta', 'though', 'bed', 'yet', 'high', 'gold', 'ask', 'ahead', 'girlfriend', 'ends', 'holla', 'tryin', 'number', 'dear', 'happening', 'picture', 'seems', 'tears', 'share', 'stare', 'couldnt', 'huh', 'gift', 'given', 'brand', 'attention', 'save', 'city', 'lay', 'eye', 'children', 'wall', 'adore', 'hurts', 'loving', 'clouds', 'miles', 'heartbreaker', 'swim', 'starts', 'drowning', 'drinkin', 'deeper', 'hangin', 'passion', 'near', 'wishing', 'figure', 'shy', 'darling', 'voice', 'hesitate', 'kind', 'fit', 'un', 'ver', 'boca', 'bom', 'suave', 'suavecito', 'nos', 'vamos', 'pegando', 'forgiveness', 'blame', 'game', 'quit', 'ring', 'shake', 'street', 'school', 'knows', 'everyone', 'admit', 'job', 'problem', 'low', 'dj', 'early', 'mornin', 'dawn', 'sick', 'imitators', 'intimidate', 'watchin', 'dreams', 'cake', 'table', 'plug', 'hair', 'door', 'straight', 'silver', 'water', 'swag', 'spend', 'after', 'aye', 'strength', 'sea', 'days', 'blind', 'hungry', 'insecure', 'lovely', 'laugh', 'blast', 'may', 'equation', 'serious', 'meant', 'middle', 'word', 'lot', 'most', 'empty', 'year', 'soul', 'purpose', 'honest', 'saw', 'faces', 'started', 'learning', 'starting', 'old', 'round', 'shoulder', 'somehow', 'road', 'behind', 'today', 'afraid', 'prized', 'possession', 'ocean', 'necessary', 'overboard', 'whoah', 'drake', 'whose', 'wear', 'sleeve', 'tries', 'icy', 'recognition', 'indecisive', 'win', 'breathe', 'laughing', 'minute', 'bottom', 'lead', 'direction', 'thankful', 'slowly', 'key', 'tú', 'plan', 'con', 'para', 'si', 'estás', 'conmigo', 'le', 'enseñes', 'lugares', 'favoritos', 'hasta', 'provocar', 'gritos', 'olvides', 'apellido', 'es', 'esa', 'shot', 'once', 'shout', 'item', 'playing', 'compared', 'starstruck', 'woke', 'daily', 'starbucks', 'pound', 'skip', 'playground', 'weekend', 'amazing', 'crying', 'caught', 'small', 'fell', 'walls', 'hot', 'step', 'bonnie', 'clyde', 'ice', 'went', 'watching', 'bitch', 'roll', 'boy', 'starving', 'homeless', 'platinum', 'child', 'wings', 'rather', 'fire', 'gentleman', 'treat', 'chick', 'toe', 'rewind', 'burn', 'reach', 'destiny', 'jb', 'hes', 'older', 'stars', 'plans', 'sad', 'buyin', 'gifts', 'funny', 'getting', 'holding', 'separate', 'praying', 'journey', 'gave', 'meaning', 'woo', 'werent', 'compass', 'woa', 'loves', 'fill', 'nights', 'drink', 'reality', 'doubters', 'watch', 'streets', 'merry', 'kept', 'cards', 'girlone', 'wasnt', 'dinner', 'losin', 'luda', 'faster', 'walks', 'lied', 'addicted', 'headache', 'twisted', 'explosive', 'learned', 'team', 'part', 'hurting', 'steel', 'human', 'act', 'means', 'says', 'speed', 'drunk', 'nice', 'belonged', 'finer', 'itd', 'sunshine', 'vision', 'games', 'mhm', 'instead', 'deceiving', 'delusional', 'later', 'careful', 'space', 'gray', 'hm', 'blue', 'remnant', 'knocked', 'tall', 'ish', 'yah', 'oooo', 'possibility', 'connect', 'acknowledge', 'successful', 'recovery', 'bless', 'trial', 'error', 'x4', 'complaining', 'overprotective', 'leaving', 'compromise', 'preaching', 'ours', 'confusing', 'straightforward', 'candles', 'ran', 'wandered', 'chinatown', 'whered', 'wheres', 'whys', 'fault', 'frozen', 'havent', 'spoken', '3', 'flight', 'voicemail', 'g', 'meantime', 'invite', 'guaranteeing', 'loyalty', 'echo', 'shook', 'achieve', 'such', 'eres', 'voy', 'está', 'más', 'normal', 'esto', 'respirar', 'cuello', 'deja', 'diga', 'cosas', 'al', 'oído', 'acuerdes', 'desnudarte', 'besos', 'firmo', 'en', 'las', 'paredes', 'laberinto', 'hacer', 'cuerpo', 'todo', 'manuscrito', 'bailar', 'pelo', 'ser', 'ritmo', 'déjame', 'sobrepasar', 'zonas', 'peligro', 'ven', 'sabes', 'amor', 'tengo', 'cuando', 'twice', 'redeem', 'chances', 'piece', 'arent', 'anyone', 'movin', 'sleeping', 'tried', 'khaled', 'spot', 'drops', 'hopscotch', 'prr', 'met', 'frame', 'nigga', 'blow', 'forgive', 'record', 'winning', 'billion', 'fighting', 'bridge', 'skys', 'view', 'chillin', 'pre', 'bright', 'laying', 'calling', 'mr', 'seem', 'type', 'disappear', 'dancin', 'paradise', 'moe', 'higher', 'turnin', 'attack', 'power', 'handle', 'bigger', 'bit', 'push', 'comes', 'dime', 'belong', 'thatd', 'falls', 'needed', 'replaced', 'doin', 'harder', 'talking', 'shouldve', 'staying', 'ended', 'miracle', 'grace', 'gas', 'showed', 'midst', 'doubt', 'doesnt', 'comin', 'fraction', 'fills', 'reply', 'press', 'taking', 'feet', 'begging', 'complementing', 'jeans', 'beats', 'hop', 'louis', 'choose', 'nono', 'lift', 'grown', 'nothings', 'spreadin', 'cheer', 'holiday', 'starin', 'everyones', 'star', 'underneath', 'breathing', 'walking', 'promises', 'grateful', 'spent', 'telling', 'happen', 'yous', 'either', 'thirteen', 'dazin', 'amazin', 'breakin', 'focused', 'foreign', 'stopping', 'dancing', 'sexual', 'romancing', 'nasty', 'fancy', 'nationality', 'wonder', 'twerk', 'doctors', 'boat', 'goes', 'blank', 'pressures', 'lightning', 'lately', 'drinking', 'ticket', 'somewhere', 'bodys', 'decision', 'ourselves', 'house', 'kids', 'thirty', 'twelve', 'bitches', 'beast', 'distant', 'closing', 'wondering', 'dark', 'knowing', 'sitting', 'spread', 'heard', 'runnerup', 'selfish', 'lesson', 'pulling', 'notorious', 'full', 'hollow', 'sugar', 'kill', 'jagged', 'pill', 'swallow', 'mountains', 'bumps', 'upside', 'soon', 'thinkin', 'hopin', 'secret', 'seasons', 'speak', 'uhoh', 'texts', 'worried', 'king', 'queen', 'distance', 'gets', 'reckless', 'clumsy', 'ones', 'surface', 'pay', 'almost', 'none', 'must', 'patient', 'wait', 'foot', 'weakness', 'shouldnt', 'offers', 'unconditional', 'itll', 'ins', 'willing', 'able', 'fold', 'command', 'permanent', 'stain', 'wishin', 'wash', 'believing', 'livid', 'moonlight', 'skies', 'galaxy', 'reaching', 'believed', 'season', 'sprung', 'beating', 'drama', 'sing', 'slipping', 'wasted', 'faded', 'fuck', 'created', 'eat', 'window', 'driveway', 'safe', 'dumb', 'chest', 'keeping', 'visionary', 'generation', 'inspiration', 'twenty', 'dedication', 'occasion', 'condone', 'component', 'opponent', 'seize', 'guitar', 'simple', 'growing', 'momma', 'telephone', 'invisible', 'deepest', 'immature', 'endure', 'hopefully', 'romance', 'yay', 'ghetto', 'cray', 'ache', 'x5', 'blessin', 'situation', 'sunrise', 'darkest', 'savor', 'tailormade', 'nibble', 'ear', 'imán', 'soy', 'metal', 'acercando', 'armando', 'sólo', 'pensarlo', 'se', 'acelera', 'pulso', 'gustando', 'lo', 'todos', 'mis', 'sentidos', 'van', 'pidiendo', 'hay', 'tomarlo', 'sin', 'ningún', 'apuro', 'pido', 'beso', 'dámelo', 'sé', 'pensándolo', 'llevo', 'tiempo', 'intentándolo', 'mami', 'dando', 'dándolo', 'corazón', 'hace', 'beba', 'buscando', 'prueba', 'cómo', 'sabe', 'cuánto', 'ti', 'cabe', 'prisa', 'dar', 'viaje', 'empecemos', 'lento', 'después', 'salvaje', 'besas', 'destreza', 'veo', 'malicia', 'delicadeza', 'belleza', 'rompecabezas', 'pero', 'pa', 'montarlo', 'aquí', 'pieza', '¡oye', 'puerto', 'rico', 'screaming', '¡ay', 'bendito', 'esté', 'contigo', '¡bailalo', 'fonsi', 'dy', 'angry', 'honesty', 'apologies', 'referee', 'hundred', 'innocent', 'spill', 'looked', 'losing', '13', 'dazing', 'parade', 'clubs', 'using', 'goodness', 'sake', 'hated', 'opinion', 'vulnerable', 'quavo', 'crisscross', 'blocks', 'looks', 'makeup', 'commas', 'fame', 'bus', 'famous', 'modern', 'named', 'angles', 'chanel', 'price', 'coochie', 'melt', 'club', 'asked', 'booty', 'gucci', 'belt', 'label', 'bottles', 'tables', 'netflix', 'cable', 'fuckin', 'everybody', 'response', 'turning', 'bronze', 'molly', 'zombie', 'whitney', 'bobby', 'tunechi', 'finessin', 'legend', 'crescent', 'bae', 'essence', 'spinnin', 'records', 'mula', 'gang', 'flexing', 'exes', 'seven', 'frowning', 'cruel', 'soldier', 'destinys', 'scene', 'sean', 'yobig', 'butyoure', 'hallelujah', 'placeill', 'rendezvous', 'itill', 'spell', 'limit', 'stepping', 'cameras', 'shoot', 'bother', 'grass', 'greener', 'green', 'knowwe', 'places', 'eatin', 'fondue', 'dunno', 'hello', 'falsetto', 'three', 'buzz', 'lightyear', 'globe', 'burr', 'epic', 'spin', 'twirl', 'whirl', 'swaggie', 'week', 'hollas', 'decide', 'keeps', 'cmon', 'searching', 'missin', 'dice', 'guys', 'return', 'climb', 'highest', 'tower', 'jsmith', 'gotcha', 'lil', 'bro', 'aight', 'taller', 'stronger', 'longer', 'chill', 'sour', 'thrill', 'pun', 'intended', 'raised', 'luke', 'force', 'shove', 'kobe', 'fourth', 'blood', 'fliest', 'david', 'goliath', 'conquered', 'giant', 'born', 'land', 'butterflies', 'stomach', 'struggle', 'climbin', 'mountain', 'heartis', 'humbles', 'troubles', 'trouble', 'called', 'chosen', 'sucker', 'leavin', 'refuse', 'disappeared', 'everybodys', 'rumors', 'spreading', 'does', 'use', 'movies', 'takin', 'disarm', 'shield', 'flowers', 'hours', 'ohohohohh', 'lasts', 'uuh', 'brings', 'anybody', 'crossroad', 'which', 'treadmill', 'hazard', 'hoping', 'wholl', 'dollars', 'quarter', 'tank', 'truck', 'read', 'map', 'relationship', 'ski', 'slope', 'avalanche', 'salvage', 'blizzard', 'april', 'skating', 'thin', 'scream', 'holler', 'tearing', 'affection', 'perception', 'crucify', 'deny', 'reputations', 'line', 'judge', 'feeds', 'years', 'chase', 'cot', 'rise', 'imagine', 'aching', 'jealous', 'females', 'hatin', 'complications', 'elevation', 'conversation', 'obligations', 'loose', 'outlaw', 'hug', 'chasing', 'badly', 'chasin', 'hesitation', 'reservation', 'shed', 'tear', 'bodies', 'touching', 'desire', 'kissing', 'neck', 'quiet', 'benz', 'cruise', 'snooze', 'v', 'shoes', 'secrets', 'clues', 'choice', 'whatd', 'hmm', 'gatherin', 'chestnuts', 'roastin', 'july', 'folks', 'santas', 'reindeer', 'flyin', 'list', 'wisemen', 'steps', 'wept', 'heres', 'blessed', 'journeys', 'sending', 'farewell', 'peace', 'sins', 'spirits', 'ease', 'lessons', 'wherever', 'necessarily', 'position', 'decisions', 'cheating', 'weak', 'understanding', 'overs', 'shoulders', 'cried', 'bags', 'packed', 'ors', 'thered', 'girlim', '14th', 'february', 'dates', 'plates', 'food', 'torn', 'photographs', 'taping', 'therell', 'locked', 'fantasy', 'front', 'passports', 'lipstick', 'satin', 'sheets', 'smell', 'perfume', 'notice', 'hypnotized', 'moves', 'mona', 'lisa', 'masterpiece', 'tattoos', 'piercings', 'brains', 'curve', 'earn', 'berb', 'legal', 'note', 'pockets', 'clothes', 'fitted', 'diddy', 'mate', 'bicycle', 'planes', 'trains', 'chains', 'icicles', 'honey', 'bunny', 'goodfella', 'movie', 'nonsense', 'thinkingthinking', 'knewyeah', 'replace', 'relate', 'confusion', 'illusion', 'anywhere', 'deal', 'zero', 'prove', 'convenience', 'store', 'managed', 'cross', 'border', 'jobs', 'mans', 'bottle', 'somebodys', 'cruising', 'entertain', 'market', 'checkout', 'promoted', 'shelter', 'suburbs', 'pays', 'bills', 'bar', 'hoped', 'nicki', 'minaj', 'couldve', 'bought', 'bow', 'hottest', 'ink', 'lines', 'incline', 'tour', 'ten', 'letters', 'sign', 'ether', 'buns', 'wiener', 'selena', 'east', 'confessions', 'priest', 'deceased', 'solid', 'mommy', 'daddy', 'town', 'papers', 'tv', 'everywhere', 'soldiers', 'dying', 'appetite', 'starve', 'sinner', 'plate', 'brokenhearted', 'lungs', 'souls', 'story', 'friend', 'joy', 'aware', 'wanting', 'friendship', 'forgot', 'breaks', 'rekindle', 'fade', 'coolest', 'prettier', 'rest', 'wished', 'impress', 'wrongs', 'fairy', 'tale', 'rules', 'behave', 'basically', 'contemplating', 'anatomy', 'gravity', 'least', 'sinking', 'disaster', 'borderline', 'despite', 'imperfections', 'hasnt', 'personal', 'umbrella', 'ohoh', 'meanwhile', 'presence', 'blessings', 'daylight', 'awake', 'biased', 'significant', 'bothers', 'existence', 'faithful', 'symphony', 'violins', 'sink', 'march', 'sweep', 'romeo', 'juliet', 'bet', 'hide', 'fun', 'adam', 'eve', 'tragedy', 'sunny', 'cher', 'fightin', 'inch', 'fiber', 'foolish', 'disagree', 'reminiscin', 'conventions', 'dynamic', 'duos', 'imperfectly', 'itching', 'scratching', 'reverse', 'battles', 'wins', 'reflect', 'revolves', 'waking', 'vacant', 'driven', 'boo', 'million', 'brighter', 'focus', 'limelight', 'emotions', 'beginning', 'effect', 'literally', 'angels', 'incomplete', 'apparently', 'bood', 'rolled', 'flame', 'runnin', 'remnants', 'instance', 'blink', 'goodbye', 'notion', 'vain', 'percussions', 'ghost', 'memories', 'serenity', 'dwindling', 'faith', 'clear', 'doubting', 'foldin', 'birthday', 'sayll', 'raining', 'equations', 'solve', 'quick', 'numbers', 'hows', 'allow', 'oceans', 'weather', 'wetter', 'wearer', 'history', 'sweater', 'serendipity', 'blew', 'dude', 'fortytwo', 'accomplished', 'goals', 'wraith', 'retire', 'pimp', 'superfly', 'cape', 'mets', 'cap', 'badge', 'esco', 'ready', 'quits', 'lace', 'pearls', 'swimming', 'galaxies', 'underground', 'forth', 'understood', 'halfway', 'short', 'anymore', 'yeahh', 'wassup', 'excuse', 'shoulda', 'white', 'cups', 'purple', 'pink', 'pending', 'mix', 'yeahhh', 'slippping', 'drizzy', 'check', 'mothafucking', 'north', 'kick', 'court', 'acting', 'bunch', 'certain', 'shit', 'excite', 'yall', 'their', 'cell', 'phones', 'pm', 'niggas', 'hating', 'probably', 'whoatrust', 'calls', 'add', 'realizing', 'calm', 'paper', 'delirious', 'conscience', 'few', 'ignition', 'stretch', 'crosslights', 'hoes', 'golf', 'frostbite', 'livin', 'sleepin', 'solo', 'loco', 'uno', 'yoko', 'ono', 'ryu', 'ken', 'argued', 'eight', 'masseuses', 'cookie', 'lucious', 'waist', 'heal', 'headlights', 'pacing', 'hallway', 'urge', 'pin', 'drop', 'cept', 'clock', 'fireside', 'sit', 'awayget', 'regret', 'hardest', 'outro', 'mmmm', 'billionaire', 'regular', 'five', 'strings', 'string', 'melody', 'mood', 'swing', 'quickly', 'disappoint', 'disappoints', 'rollercoaster', 'beach', 'vacation', 'sittin', 'walked', 'dragon', 'sees', 'effort', 'questions', 'drain', 'pains', 'along', 'changes', 'ought', 'wakes', 'nervous', 'contagious', 'mirror', 'saved', 'understands', 'gangsta', 'thank', 'shines', 'breezy', 'commit', 'attractive', 'mission', 'crib', 'wow', 'carnival', 'motha', 'circus', 'olay', 'telly', 'cali', 'dough', 'fake', 'vip', 'c', 'lac', 'speakers', 'chitchat', 'hotties', 'pilates', 'knee', 'intentions', 'caused', 'pushed', 'motions', 'changing', 'smoking', 'backpack', 'snapback', 'rap', 'wearing', 'heels', 'asking', 'cccanada', 'vuittons', 'curves', 'flyest']\n",
            "my sending mistakes fine i us always the less never i ill be down its of i let on in floor in i uh 1 no home the lonely take now\n"
          ]
        }
      ],
      "source": [
        "# Print the words in the vocabulary\n",
        "print(\"Words in the vocabulary:\", list(vocab_to_int.keys()))\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_lyrics(model, start_words, length=30, temperature=1.0):\n",
        "    model.eval()\n",
        "\n",
        "    # Convert input to lowercase to match preprocessed vocabulary\n",
        "    words = preprocess_text(start_words.lower())\n",
        "\n",
        "    # Check if words exist in vocabulary, or skip them\n",
        "    state = [vocab_to_int[word] for word in words if word in vocab_to_int]\n",
        "\n",
        "    # Handle the case where no valid words are found in vocab\n",
        "    if len(state) == 0:\n",
        "        raise ValueError(f\"None of the words in '{start_words}' are in the vocabulary.\")\n",
        "\n",
        "    generated_words = words[:]  # Start with the input words\n",
        "\n",
        "    # Generate lyrics by predicting the next word\n",
        "    for _ in range(length):\n",
        "        # Ensure the tensor is of type Long (as required by the embedding layer)\n",
        "        state_tensor = torch.tensor([state[-seq_length:]], dtype=torch.long)\n",
        "        output = model(state_tensor)\n",
        "\n",
        "        # Apply temperature to the output logits\n",
        "        output = output / temperature\n",
        "        probabilities = F.softmax(output, dim=1)\n",
        "\n",
        "        # Sample from the probability distribution instead of argmax\n",
        "        predicted_idx = torch.multinomial(probabilities, 1).item()\n",
        "        state.append(predicted_idx)\n",
        "\n",
        "        # Append the generated word\n",
        "        generated_words.append(vocab[predicted_idx])\n",
        "\n",
        "    return ' '.join(generated_words)\n",
        "\n",
        "# Test with temperature sampling\n",
        "start_words = \"my\"\n",
        "generated_lyrics = generate_lyrics(model, start_words, temperature=0.8)  # Experiment with temperature\n",
        "print(generated_lyrics)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XE0XMN66miYf"
      },
      "id": "XE0XMN66miYf",
      "execution_count": 12,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}